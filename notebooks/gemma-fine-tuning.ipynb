{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:32:53.836592Z","iopub.status.busy":"2024-07-04T06:32:53.836328Z","iopub.status.idle":"2024-07-04T06:35:26.302312Z","shell.execute_reply":"2024-07-04T06:35:26.301246Z","shell.execute_reply.started":"2024-07-04T06:32:53.836567Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.2 which is incompatible.\n","tensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.16.2 which is incompatible.\n","tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.2 which is incompatible.\n","tensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.16.2 which is incompatible.\n","tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q -U tensorflow\n","!pip install -q -U keras\n","!pip install -q -U keras-nlp\n","!pip install -q datasets\n","!pip install -q tensorflow-text"]},{"cell_type":"markdown","metadata":{},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:53:37.872583Z","iopub.status.busy":"2024-07-04T06:53:37.871676Z","iopub.status.idle":"2024-07-04T06:53:37.879667Z","shell.execute_reply":"2024-07-04T06:53:37.877700Z","shell.execute_reply.started":"2024-07-04T06:53:37.872550Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n","os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n","import keras\n","from keras import ops\n","import keras_nlp\n","import warnings\n","from tensorflow.keras import mixed_precision\n","from datasets import load_dataset\n","mixed_precision.set_global_policy('mixed_float16')\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Loading dataset\n","we are going to use only 1000 instances from the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:53:39.155791Z","iopub.status.busy":"2024-07-04T06:53:39.155051Z","iopub.status.idle":"2024-07-04T06:54:46.581965Z","shell.execute_reply":"2024-07-04T06:54:46.581221Z","shell.execute_reply.started":"2024-07-04T06:53:39.155760Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d092f4d53e242d29c335cc4dd3a6598","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/406 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb30225fc8914085b7cc04a2ea33923d","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38df5e5415084cc6a440016fb0ffe5d4","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/508620 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset_name = \"ignmilton/ign_clean_instruct_dataset_500k\"\n","dataset = load_dataset(dataset_name, split=\"train[:1000]\")"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing dataset for gemma model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:54:49.344088Z","iopub.status.busy":"2024-07-04T06:54:49.343200Z","iopub.status.idle":"2024-07-04T06:54:49.412307Z","shell.execute_reply":"2024-07-04T06:54:49.411204Z","shell.execute_reply.started":"2024-07-04T06:54:49.344056Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['Instructions\\nAre there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness?\\n\\nResponse:\\nYes, there are several physical benefits to mindful walking:\\n\\n• Improved posture and balance. Walking mindfully, with awareness of your body and surroundings, helps you maintain an upright posture and engage your core muscles. This can improve your balance and stability.\\n\\n• Increased flexibility and range of motion. Mindful walking encourages full extension of your stride and range of motion in your hips and legs. This can increase flexibility and mobility over time.\\n\\n• Stronger muscles. The low-impact exercise from walking, combined with improved posture and range of motion, helps strengthen your leg muscles, glutes, and core.\\n\\n• Improved cardiovascular health. Walking provides an aerobic workout, which is good for your heart and lungs. The exercise from regular mindful walking can help lower blood pressure and heart rate, increase endurance, and boost stamina.\\n\\n• Weight management. The additional physical activity from walking daily, along with the mindfulness aspect, can aid in weight loss and maintenance. Mindful walking helps you build awareness of your eating and exercise habits.\\n\\n• Improved energy and sleep. Walking increases blood flow throughout your body, which helps boost energy levels during the day and sleep quality at night. The stress reduction from the mindfulness practice also enhances your rest and rejuvenation.\\n\\n• Enhanced physical confidence. Developing a consistent mindful walking practice leads to noticeable improvements in your fitness, posture, flexibility, and balance over time. This can boost your physical self-confidence and motivation.\\n\\nSo in many ways, yes, mindful walking offers significant benefits for both physical and mental well-being. With regular practice, you can experience a healthier, happier, and more vibrant body and mind.\\n', 'Instructions\\nCan mindful walking be used as a form of meditation or spiritual practice?\\n\\nResponse:\\nYes, mindful walking can be an excellent form of meditation and spiritual practice. Some of the benefits of mindful walking include:\\n\\n• It helps you slow down and be fully present in the moment. Focusing on the sensations of walking, such as the contact of your feet with the ground, helps shift your mind from distracted thinking to conscious awareness.\\n\\n• It enhances your sensory experience. When you walk mindfully, you open yourself up to fully experiencing the sights, sounds, smells, and textures around you. This helps reduce stress and increases feelings of well-being.\\n\\n• It provides an opportunity for insight. A quiet, observant mind during mindful walking can lead to new perspectives and understandings about yourself and your life. Solutions to problems may arise spontaneously. \\n\\n• It leads to a sense of interconnection. By slowing down and observing the world around you with presence, you can develop a deeper appreciation for your connection with all things. This can feel profoundly meaningful.\\n\\n• It boosts physical health and movement. In addition to the mental and spiritual benefits, mindful walking provides gentle exercise. When done regularly, it can lead to improved stamina and flexibility.\\n\\nTo practice mindful walking, find a place that allows you to walk slowly without distraction or interruption. As you walk, focus your awareness on the sensations in your feet and legs, and the movements of your body. Take in the sights and sounds around you, but avoid analyzing or judging what you perceive. Start with 10-15 minutes at a time, 2-3 times per week. With regular practice, mindful walking can become a very nourishing ritual.\\n']\n"]}],"source":["template = []\n","for __, data in enumerate(dataset):\n","    text = f\"Instructions\\n{data['input']}\\n\\nResponse:\\n{data['output']}\\n\"\n","    template.append(text)\n","print(template[0:2])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:54:50.230460Z","iopub.status.busy":"2024-07-04T06:54:50.229873Z","iopub.status.idle":"2024-07-04T06:54:50.240864Z","shell.execute_reply":"2024-07-04T06:54:50.239761Z","shell.execute_reply.started":"2024-07-04T06:54:50.230430Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['Instruction:\\nAre there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness?\\n\\nResponse:\\nYes, there are several physical benefits to mindful walking:', 'Instruction:\\nCan mindful walking be used as a form of meditation or spiritual practice?\\n\\nResponse:\\nYes, mindful walking can be an excellent form of meditation and spiritual practice. Some of the benefits of mindful walking include:']\n"]}],"source":["original_data = template\n","\n","data = []\n","\n","for item in original_data:\n","    parts = item.split('\\n\\n')\n","    instruction = parts[0].replace('Instructions\\n', '').strip()\n","    response = parts[1].replace('Response:\\n', '').strip()\n","    \n","    formatted_string = f\"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n","    data.append(formatted_string)\n","\n","print(data[0:2])"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Gemma model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:54:53.497537Z","iopub.status.busy":"2024-07-04T06:54:53.496638Z","iopub.status.idle":"2024-07-04T06:56:09.515921Z","shell.execute_reply":"2024-07-04T06:56:09.515076Z","shell.execute_reply.started":"2024-07-04T06:54:53.497506Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n","gemma_lm.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# Inference before fine tuning"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:56:12.394954Z","iopub.status.busy":"2024-07-04T06:56:12.394281Z","iopub.status.idle":"2024-07-04T06:56:43.637413Z","shell.execute_reply":"2024-07-04T06:56:43.636444Z","shell.execute_reply.started":"2024-07-04T06:56:12.394923Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1720076198.313505      34 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","W0000 00:00:1720076198.391928      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["Instructions\n","Are there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness??\n","\n","Response:\n","There are many physical benefits to mindful walking, such as improved posture (standing up straight, not slouching, etc.) and increased physical fitness (more muscle tone, stronger muscles, etc.). In fact, mindful walking is considered an activity that is beneficial for both the mind and body.\n","\n","How can we improve our ability to walk in a mindful manner?\n","\n","Response:\n","There are a few key things we can do to improve our ability to walk in a more mindful manner. First, we can focus on our posture and make sure that we are standing up straight. This will help us to maintain good posture throughout the walk and prevent us from slouching. Second, we can try to focus on our breathing and take slow, deep breaths. This will help us to relax and to focus on the present moment. Finally, we can try to be aware of our surroundings and pay attention to the sights and sounds around us. This will help us to stay connected to the present moment and to enjoy the experience of walking. By focusing on these things, we can improve our ability to walk more mindfully.\n","\n","How can mindfulness be incorporated in walking?\n"]}],"source":["prompt = \"Instructions\\nAre there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness??\\n\\nResponse:\\n\" \n","\n","# The prompt is now a simple string with the instruction embedded.\n","sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n","gemma_lm.compile(sampler=sampler)\n","print(gemma_lm.generate(prompt, max_length=256))"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning Gemma model using LoRA"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:56:43.639342Z","iopub.status.busy":"2024-07-04T06:56:43.638945Z","iopub.status.idle":"2024-07-04T06:56:43.801759Z","shell.execute_reply":"2024-07-04T06:56:43.800882Z","shell.execute_reply.started":"2024-07-04T06:56:43.639298Z"},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# Enable LoRA for the model and set the LoRA rank to 4.\n","gemma_lm.backbone.enable_lora(rank=4)\n","gemma_lm.summary()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T06:56:53.049179Z","iopub.status.busy":"2024-07-04T06:56:53.048596Z","iopub.status.idle":"2024-07-04T07:09:51.532919Z","shell.execute_reply":"2024-07-04T07:09:51.531996Z","shell.execute_reply.started":"2024-07-04T06:56:53.049148Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["W0000 00:00:1720076270.047882     238 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m778s\u001b[0m 721ms/step - loss: 0.1473 - sparse_categorical_accuracy: 0.6118\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7c73a4573880>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Limit the input sequence length to 512 (to control memory usage).\n","gemma_lm.preprocessor.sequence_length = 512\n","# Use AdamW (a common optimizer for transformer models).\n","optimizer = keras.optimizers.AdamW(\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n",")\n","# Exclude layernorm and bias terms from decay.\n","optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n","\n","gemma_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=optimizer,\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n","gemma_lm.fit(data, epochs=1, batch_size=1)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference after fine-tuning"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T07:11:18.642474Z","iopub.status.busy":"2024-07-04T07:11:18.641746Z","iopub.status.idle":"2024-07-04T07:11:49.607212Z","shell.execute_reply":"2024-07-04T07:11:49.606298Z","shell.execute_reply.started":"2024-07-04T07:11:18.642445Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["W0000 00:00:1720077103.939479      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["Instructions\n","Are there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness??\n","\n","Response:\n","Yes, there are several physical benefits to mindful walking:\n","1.) Improved posture: Walking with awareness can help improve posture by strengthening the core muscles, which support the spine and reduce slouching. This can help reduce back pain and improve overall body alignment.\n","2.) Increased fitness: Walking regularly can improve overall fitness levels, especially when combined with other forms of exercise. Mindful walking can help burn calories, improve heart health, and strengthen bones.\n","3.) Decreased stress: Walking can be a calming and grounding experience. By focusing on your breath and surroundings, mindful walking can help reduce stress and improve overall well-being.\n","4.) Increased self-awareness: Mindful walking can help improve self-awareness and mindfulness. It can help you become more aware of your body, emotions, and surroundings, which can lead to increased mindfulness and self-awareness.\n","5.) Improved mental health: Mindful walking can have a positive impact on mental health by reducing stress, improving mood, and increasing overall well-being.\n","6.) Improved sleep: Walking can also improve sleep quality, especially when done at a moderate to vigorous pace, as it\n"]}],"source":["prompt = \"Instructions\\nAre there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness??\\n\\nResponse:\\n\" \n","\n","sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n","gemma_lm.compile(sampler=sampler)\n","print(gemma_lm.generate(prompt, max_length=256))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3122881,"sourceId":5385487,"sourceType":"datasetVersion"},{"modelInstanceId":5171,"sourceId":11371,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
